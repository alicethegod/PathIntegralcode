{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Fitting and Cognitive Theory Analysis\n",
    "\n",
    "This script uses a PyTorch neural network to fit a noisy cosine curve, aiming to create and analyze the phenomena of \"underfitting,\" \"good fitting,\" and \"overfitting.\"\n",
    "\n",
    "**Core Highlights:**\n",
    "\n",
    "1.  Uses neural network models of varying complexity to correspond to the three fitting states.\n",
    "2.  For each trained model, it employs the `PathIntegralAnalyzer` to calculate the **Cognitive Internal Energy (U)** and **Entropy (S)** metrics based on the latest path integral physics theory.\n",
    "3.  Visualizes the model's fitting performance alongside the theoretical analysis metrics, intuitively demonstrating the relationship between the model's external behavior and its internal structural measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# --- 1. Data Preparation ---\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "np.random.seed(0)\n",
    "n_samples = 30\n",
    "X_np = np.sort(np.random.rand(n_samples))\n",
    "y_np = true_fun(X_np) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_tensor = torch.FloatTensor(X_np).unsqueeze(1)\n",
    "y_tensor = torch.FloatTensor(y_np).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Neural Network Model Definition ---\n",
    "class RegressionMLP(nn.Module):\n",
    "    \"\"\"A Multi-Layer Perceptron for regression tasks.\"\"\"\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(RegressionMLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes) - 2):\n",
    "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(layer_sizes[-2], layer_sizes[-1]))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Path Integral Physics Theory Analyzer ---\n",
    "class PathIntegralAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes a neural network based on the path integral physics theory.\n",
    "    Calculates Cognitive Internal Energy (U) and Entropy (S).\n",
    "    - Grounding Node: The single output node of the regression model.\n",
    "    - Graph Construction: Can handle any number of layers in the RegressionMLP.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        model_copy = copy.deepcopy(model)\n",
    "        self.model = model_copy.to('cpu')\n",
    "        self.linear_layers = [l for l in self.model.layers if isinstance(l, nn.Linear)]\n",
    "        self.graph = self._build_graph()\n",
    "        # For a regression task, the grounding node is the unique output node\n",
    "        self.grounding_nodes = {f\"{len(self.linear_layers)}-0\"}\n",
    "        self.hidden_nodes = self._get_hidden_nodes()\n",
    "        self.memoized_paths = {}\n",
    "\n",
    "    def _build_graph(self):\n",
    "        G = nx.DiGraph()\n",
    "        # Input layer nodes\n",
    "        for i in range(self.linear_layers[0].in_features):\n",
    "            G.add_node(f\"0-{i}\", layer=0)\n",
    "        # Hidden and output layer nodes\n",
    "        for i, l in enumerate(self.linear_layers):\n",
    "            for j in range(l.out_features):\n",
    "                G.add_node(f\"{i+1}-{j}\", layer=i+1)\n",
    "            weights = torch.abs(l.weight.data.t())\n",
    "            probs = torch.softmax(weights, dim=1)\n",
    "            for u in range(l.in_features):\n",
    "                for v in range(l.out_features):\n",
    "                    p = probs[u, v].item()\n",
    "                    if p > 1e-9:\n",
    "                        # Cost represents the energy of propagating along an edge\n",
    "                        G.add_edge(f\"{i}-{u}\", f\"{i+1}-{v}\", cost=1.0 - np.log(p))\n",
    "        return G\n",
    "\n",
    "    def _get_hidden_nodes(self):\n",
    "        # Hidden nodes are all nodes except the input (0) and output (len) layers\n",
    "        num_layers = len(self.linear_layers)\n",
    "        return [n for n, d in self.graph.nodes(data=True) if 0 < d['layer'] < num_layers]\n",
    "\n",
    "    def find_all_paths_dfs(self, start, targets):\n",
    "        memo_key = (start, tuple(sorted(list(targets))))\n",
    "        if memo_key in self.memoized_paths: return self.memoized_paths[memo_key]\n",
    "\n",
    "        paths, stack = [], [(start, [start], 0)]\n",
    "        while stack:\n",
    "            curr, path, cost = stack.pop()\n",
    "            if curr in targets:\n",
    "                paths.append({'path': path, 'cost': cost})\n",
    "                continue\n",
    "            if len(path) > (len(self.linear_layers) + 2): continue # Limit path depth\n",
    "            for neighbor in self.graph.neighbors(curr):\n",
    "                if neighbor not in path:\n",
    "                    stack.append((neighbor, path + [neighbor], cost + self.graph[curr][neighbor]['cost']))\n",
    "        self.memoized_paths[memo_key] = paths\n",
    "        return paths\n",
    "\n",
    "    def calculate_metrics_for_node(self, node):\n",
    "        paths = self.find_all_paths_dfs(node, self.grounding_nodes)\n",
    "        if not paths: return float('inf'), float('inf')\n",
    "\n",
    "        costs = np.array([p['cost'] for p in paths])\n",
    "        \n",
    "        # Cognitive Internal Energy (U) - Harmonic mean of path energies (costs)\n",
    "        conductances = 1.0 / (costs + 1e-9)\n",
    "        U = 1.0 / np.sum(conductances) if np.sum(conductances) > 0 else float('inf')\n",
    "\n",
    "        # Entropy (S) - Shannon entropy of path importances\n",
    "        importances = np.exp(-1.0 * costs)\n",
    "        groundingness = np.sum(importances)\n",
    "        probabilities = importances / groundingness if groundingness > 0 else []\n",
    "        if probabilities.size > 0:\n",
    "             S = -np.sum(probabilities * np.log2(probabilities + 1e-9))\n",
    "        else:\n",
    "            S = float('inf')\n",
    "\n",
    "        return U, S\n",
    "\n",
    "    def analyze(self):\n",
    "        U_vals, S_vals = [], []\n",
    "        if not self.hidden_nodes: return 0, 0 # If no hidden layers, metrics are zero\n",
    "\n",
    "        # Analyze all hidden nodes\n",
    "        for node in tqdm(self.hidden_nodes, desc=f\"Analyzing Cognitive Metrics\", leave=False):\n",
    "            U, S = self.calculate_metrics_for_node(node)\n",
    "            if np.isfinite(U) and np.isfinite(S):\n",
    "                U_vals.append(U)\n",
    "                S_vals.append(S)\n",
    "\n",
    "        avg_U = np.mean(U_vals) if U_vals else 0\n",
    "        avg_S = np.mean(S_vals) if S_vals else 0\n",
    "        return avg_U, avg_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Main Training and Analysis Workflow ---\n",
    "\n",
    "# Define three experimental configurations\n",
    "model_configs = {\n",
    "    \"Underfitting\": {\n",
    "        \"layers\": [1, 5, 3, 1],  # Simple structure\n",
    "        \"epochs\": 1000,\n",
    "        \"lr\": 0.01\n",
    "    },\n",
    "    \"Good Fit\": {\n",
    "        \"layers\": [1, 64, 64, 1], # Moderately complex model\n",
    "        \"epochs\": 5000,\n",
    "        \"lr\": 0.01\n",
    "    },\n",
    "    \"Overfitting\": {\n",
    "        \"layers\": [1, 128, 128, 128, 1], # Very complex model\n",
    "        \"epochs\": 40000, # Train for a very long time\n",
    "        \"lr\": 0.001\n",
    "    }\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Train and analyze\n",
    "for name, config in model_configs.items():\n",
    "    print(f\"--- Training Model for: {name} ---\")\n",
    "    model = RegressionMLP(config[\"layers\"])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in tqdm(range(config[\"epochs\"]), desc=f\"Training {name}\"):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Analyze after training\n",
    "    analyzer = PathIntegralAnalyzer(model)\n",
    "    avg_U, avg_S = analyzer.analyze()\n",
    "\n",
    "    # Save results\n",
    "    results[name] = {\n",
    "        \"model\": model,\n",
    "        \"loss\": loss.item(),\n",
    "        \"U\": avg_U,\n",
    "        \"S\": avg_S\n",
    "    }\n",
    "    print(f\"Final Loss: {loss.item():.4f}, Avg Cognitive Internal Energy (U): {avg_U:.4f}, Avg Entropy (S): {avg_S:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Visualization ---\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.suptitle(\"NN Curve Fitting: Underfitting vs. Good Fit vs. Overfitting\", fontsize=18)\n",
    "\n",
    "for i, name in enumerate(model_configs.keys()):\n",
    "    ax = plt.subplot(1, 3, i + 1)\n",
    "    model = results[name][\"model\"]\n",
    "\n",
    "    # Prepare test data for plotting the curve\n",
    "    X_test = torch.FloatTensor(np.linspace(0, 1, 100)).unsqueeze(1)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "\n",
    "    # Plotting\n",
    "    ax.scatter(X_np, y_np, edgecolor=\"b\", s=20, label=\"Training Samples\")\n",
    "    ax.plot(np.linspace(0, 1, 100), true_fun(np.linspace(0, 1, 100)), 'g-', label=\"True Function\")\n",
    "    ax.plot(X_test.numpy(), y_pred.numpy(), 'r-', label=\"Model Fit\")\n",
    "\n",
    "    ax.set_xlim((0, 1))\n",
    "    ax.set_ylim((-1.5, 1.5))\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Display all metrics in the title\n",
    "    title_text = (\n",
    "        f\"{name}\\n\"\n",
    "        f\"Final MSE: {results[name]['loss']:.3f}\\n\"\n",
    "        f\"Avg Cognitive Internal Energy (U): {results[name]['U']:.3f}\\n\"\n",
    "        f\"Avg Entropy (S): {results[name]['S']:.3f}\"\n",
    "    )\n",
    "    ax.set_title(title_text, fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
